{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d639af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mojtabae/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading ASSERT-KTH/DISL:decomposed/train (streaming)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting lines: 514506it [02:48, 3048.64it/s, written=500000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] Wrote 514,506 rows to disl_token_stats/loc_decomposed.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Full-pass line counting for DISL: decomposed\n",
    "# - Streams HF dataset and writes a parquet/jsonl with: sha256, loc_total, loc_nonempty (+ optional fields)\n",
    "\n",
    "import os, json, hashlib\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional parquet\n",
    "HAVE_PARQUET = False\n",
    "try:\n",
    "    import pyarrow as pa, pyarrow.parquet as pq\n",
    "    HAVE_PARQUET = True\n",
    "except Exception:\n",
    "    HAVE_PARQUET = False\n",
    "\n",
    "DATASET_NAME   = \"ASSERT-KTH/DISL\"\n",
    "CONFIG         = \"decomposed\"                 # deduplicated split\n",
    "SPLIT          = \"train\"\n",
    "OUTPUT_DIR     = \"disl_token_stats\"           # same dir you used for tokens\n",
    "OUT_BASE       = f\"loc_{CONFIG}\"\n",
    "OUT_PATH       = os.path.join(OUTPUT_DIR, f\"{OUT_BASE}.parquet\" if HAVE_PARQUET else f\"{OUT_BASE}.jsonl\")\n",
    "PERSIST_FIELDS = [\"file_path\", \"contract_address\", \"compiler_version\"]  # include if present\n",
    "HF_TOKEN       = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\", \"ignore\")).hexdigest()\n",
    "\n",
    "def count_lines(s: str):\n",
    "    if not s:\n",
    "        return 0, 0\n",
    "    lines = s.splitlines()\n",
    "    loc_total = len(lines)\n",
    "    loc_nonempty = sum(1 for ln in lines if ln.strip())\n",
    "    return loc_total, loc_nonempty\n",
    "\n",
    "class RowWriter:\n",
    "    def __init__(self, path: str, use_parquet: bool = True, batch_size: int = 5000):\n",
    "        self.path = path\n",
    "        self.use_parquet = use_parquet and HAVE_PARQUET\n",
    "        self.batch_size = batch_size\n",
    "        self.rows = []\n",
    "        self.writer = None\n",
    "        if not self.use_parquet:\n",
    "            open(self.path, \"w\").close()  # truncate JSONL\n",
    "\n",
    "    def append(self, row: Dict):\n",
    "        self.rows.append(row)\n",
    "        if len(self.rows) >= self.batch_size:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        if not self.rows:\n",
    "            return\n",
    "        if self.use_parquet:\n",
    "            table = pa.Table.from_pylist(self.rows)\n",
    "            if self.writer is None:\n",
    "                self.writer = pq.ParquetWriter(self.path, table.schema, compression=\"zstd\")\n",
    "            self.writer.write_table(table)\n",
    "        else:\n",
    "            with open(self.path, \"a\", encoding=\"utf-8\") as f:\n",
    "                for r in self.rows:\n",
    "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "        self.rows = []\n",
    "\n",
    "    def close(self):\n",
    "        self.flush()\n",
    "        if self.writer is not None:\n",
    "            self.writer.close()\n",
    "\n",
    "print(f\"[info] Loading {DATASET_NAME}:{CONFIG}/{SPLIT} (streaming)\")\n",
    "ds = load_dataset(DATASET_NAME, CONFIG, split=SPLIT, streaming=True)\n",
    "\n",
    "writer = RowWriter(OUT_PATH, use_parquet=True, batch_size=5000)\n",
    "\n",
    "written = 0\n",
    "pbar = tqdm(ds, desc=\"Counting lines\")\n",
    "for ex in pbar:\n",
    "    src = ex.get(\"source_code\")\n",
    "    if not src:\n",
    "        continue\n",
    "    loc_total, loc_nonempty = count_lines(src)\n",
    "    row = {\n",
    "        \"sha256\": sha256_text(src),\n",
    "        \"loc_total\": int(loc_total),\n",
    "        \"loc_nonempty\": int(loc_nonempty),\n",
    "    }\n",
    "    for fld in PERSIST_FIELDS:\n",
    "        if fld in ex and ex[fld] is not None:\n",
    "            row[fld] = ex[fld]\n",
    "    writer.append(row)\n",
    "    written += 1\n",
    "    if written % 20000 == 0:\n",
    "        pbar.set_postfix_str(f\"written={written}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"[done] Wrote {written:,} rows to {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a555aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] 514,506 rows\n",
      "[saved] disl_token_stats/hexbin_tokens_vs_loc_ieee.pdf\n",
      "[saved] disl_token_stats/share_exceed_by_loc_ieee.pdf\n",
      "[headline] Overall > 4096 tokens: 31.3%\n",
      "[headline] LoC where ≥50% exceed: ~300\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Configure paths ----\n",
    "OUTPUT_DIR          = \"disl_token_stats\"\n",
    "TOKENS_PARQUET_PATH = os.path.join(OUTPUT_DIR, \"lengths_decomposed.parquet\")  # from your previous run (--persist)\n",
    "LINES_PARQUET_PATH  = os.path.join(OUTPUT_DIR, \"loc_decomposed.parquet\")      # from cell above\n",
    "CONTEXT_TOKENS      = 4096\n",
    "BIN_WIDTH_LOC       = 50       # lines per bin for the share plot\n",
    "SAMPLE_HEXBIN       = 200_000  # subsample for hexbin speed (set None to use all)\n",
    "\n",
    "# ---- Load data ----\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.endswith(\".jsonl\"):\n",
    "        return pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {path}\")\n",
    "\n",
    "df_tokens = load_table(TOKENS_PARQUET_PATH)[[\"sha256\", \"tokens\"]]\n",
    "df_lines  = load_table(LINES_PARQUET_PATH)[[\"sha256\", \"loc_total\", \"loc_nonempty\"]]\n",
    "df = df_tokens.merge(df_lines, on=\"sha256\", how=\"inner\").dropna()\n",
    "print(f\"[merge] {len(df):,} rows\")\n",
    "\n",
    "# ---- Style for IEEE single column ----\n",
    "FIGSIZE = (3.4, 2.4)  # inches (w,h)\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "# ---- (A) Hexbin: tokens vs non-empty LOC ----\n",
    "plot_df = df\n",
    "if SAMPLE_HEXBIN and len(df) > SAMPLE_HEXBIN:\n",
    "    plot_df = df.sample(SAMPLE_HEXBIN, random_state=42).copy()\n",
    "\n",
    "x = plot_df[\"tokens\"].to_numpy()\n",
    "y = plot_df[\"loc_nonempty\"].to_numpy()\n",
    "\n",
    "x_cap = np.percentile(x, 99.0)\n",
    "y_cap = np.percentile(y, 99.0)\n",
    "\n",
    "fig = plt.figure(figsize=FIGSIZE)\n",
    "ax = fig.add_subplot(111)\n",
    "hb = ax.hexbin(x, y, gridsize=50, extent=(0, x_cap, 0, y_cap), mincnt=5, bins='log')\n",
    "ax.axvline(CONTEXT_TOKENS, linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "ax.set_xlim(0, x_cap)\n",
    "ax.set_ylim(0, y_cap)\n",
    "ax.set_xlabel(\"Tokens\")\n",
    "ax.set_ylabel(\"Non-empty LoC\")\n",
    "ax.set_title(\"Token vs. LoC\")\n",
    "\n",
    "# compact colorbar without frame\n",
    "cbar = fig.colorbar(hb, ax=ax, fraction=0.08, pad=0.02)\n",
    "cbar.ax.tick_params(labelsize=7)\n",
    "\n",
    "fig.tight_layout()\n",
    "hexbin_pdf = os.path.join(OUTPUT_DIR, \"hexbin_tokens_vs_loc_ieee.pdf\")\n",
    "fig.savefig(hexbin_pdf, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[saved] {hexbin_pdf}\")\n",
    "\n",
    "# ---- (B) Share exceeding 4096 tokens by LoC bins ----\n",
    "df[\"loc_bin\"] = (df[\"loc_nonempty\"] // BIN_WIDTH_LOC) * BIN_WIDTH_LOC\n",
    "g = df.groupby(\"loc_bin\", as_index=False).agg(\n",
    "    n=(\"tokens\", \"size\"),\n",
    "    share_exceed=(\"tokens\", lambda s: float((s > CONTEXT_TOKENS).mean())),\n",
    ")\n",
    "\n",
    "# filter low-support bins (optional)\n",
    "g = g[g[\"n\"] >= 50]  # keep bins with at least 50 samples for stability\n",
    "\n",
    "fig = plt.figure(figsize=FIGSIZE)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(g[\"loc_bin\"], g[\"share_exceed\"] * 100.0, linewidth=1.2)\n",
    "\n",
    "# annotate approx LoC where ≥50% exceed context\n",
    "half = g[g[\"share_exceed\"] >= 0.5]\n",
    "if not half.empty:\n",
    "    loc50 = int(half[\"loc_bin\"].iloc[0])\n",
    "    ax.axvline(loc50, linestyle=\"--\", linewidth=1.0)\n",
    "    ax.text(loc50 * 1.02, 52, f\"≥50% exceed at ~{loc50} LoC\", va=\"bottom\", ha=\"left\")\n",
    "\n",
    "ax.set_xlabel(\"Non-empty LoC (binned)\")\n",
    "ax.set_ylabel(\"Share > 4,096 tokens (%)\")\n",
    "ax.set_title(\"Where contexts exceed token budget\")\n",
    "\n",
    "# no grid, tight\n",
    "fig.tight_layout()\n",
    "share_pdf = os.path.join(OUTPUT_DIR, \"share_exceed_by_loc_ieee.pdf\")\n",
    "fig.savefig(share_pdf, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[saved] {share_pdf}\")\n",
    "\n",
    "# ---- Print quick headline numbers ----\n",
    "overall_exceed = (df[\"tokens\"] > CONTEXT_TOKENS).mean() * 100\n",
    "print(f\"[headline] Overall > {CONTEXT_TOKENS} tokens: {overall_exceed:.1f}%\")\n",
    "if not half.empty:\n",
    "    print(f\"[headline] LoC where ≥50% exceed: ~{loc50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf29fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] 514,506 rows\n",
      "[tokens/line] median=13.06, IQR=(11.79, 14.60)\n",
      "[implied LOC at 4096] median ~ 314, IQR ~ (281, 348)\n",
      "[saved] disl_token_stats/hexbin_tokens_vs_loc_guides_ieee.pdf\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Hexbin with tokens/line guide-lines + IEEE-ready PDF\n",
    "# Requires: your persisted parquet from the earlier runs:\n",
    "#   - disl_token_stats/lengths_decomposed.parquet  (sha256, tokens, ...)\n",
    "#   - disl_token_stats/loc_decomposed.parquet      (sha256, loc_total, loc_nonempty, ...)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR          = \"disl_token_stats\"\n",
    "TOKENS_PARQUET_PATH = os.path.join(OUTPUT_DIR, \"lengths_decomposed.parquet\")\n",
    "LINES_PARQUET_PATH  = os.path.join(OUTPUT_DIR,  \"loc_decomposed.parquet\")\n",
    "OUT_PDF             = os.path.join(OUTPUT_DIR,  \"hexbin_tokens_vs_loc_guides_ieee.pdf\")\n",
    "\n",
    "CONTEXT_TOKENS      = 4096\n",
    "SAMPLE_HEXBIN       = 200_000   # for speed; set None to plot all rows\n",
    "\n",
    "# --- Load & join ---\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.endswith(\".jsonl\"):\n",
    "        return pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {path}\")\n",
    "\n",
    "df_tokens = load_table(TOKENS_PARQUET_PATH)[[\"sha256\", \"tokens\"]]\n",
    "df_lines  = load_table(LINES_PARQUET_PATH)[[\"sha256\", \"loc_nonempty\"]]\n",
    "df = df_tokens.merge(df_lines, on=\"sha256\", how=\"inner\").dropna()\n",
    "df = df[df[\"loc_nonempty\"] > 0].copy()  # avoid divide-by-zero\n",
    "print(f\"[merge] {len(df):,} rows\")\n",
    "\n",
    "# --- tokens per line stats ---\n",
    "df[\"tpl\"] = df[\"tokens\"] / df[\"loc_nonempty\"]\n",
    "k50  = df[\"tpl\"].median()\n",
    "k25  = df[\"tpl\"].quantile(0.25)\n",
    "k75  = df[\"tpl\"].quantile(0.75)\n",
    "\n",
    "def loc_at_context(k): return CONTEXT_TOKENS / k\n",
    "print(f\"[tokens/line] median={k50:.2f}, IQR=({k25:.2f}, {k75:.2f})\")\n",
    "print(f\"[implied LoC at 4096] median ~ {loc_at_context(k50):.0f}, \"\n",
    "      f\"IQR ~ ({loc_at_context(k75):.0f}, {loc_at_context(k25):.0f})\")\n",
    "\n",
    "# --- Style for IEEE single column ---\n",
    "FIGSIZE = (3.4, 2.4)  # inches\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "# --- Prepare data for hexbin (crop at p99 to keep plot readable) ---\n",
    "plot_df = df\n",
    "if SAMPLE_HEXBIN and len(plot_df) > SAMPLE_HEXBIN:\n",
    "    plot_df = plot_df.sample(SAMPLE_HEXBIN, random_state=42)\n",
    "\n",
    "x = plot_df[\"tokens\"].to_numpy()\n",
    "y = plot_df[\"loc_nonempty\"].to_numpy()\n",
    "x_cap = np.percentile(df[\"tokens\"], 99.0)      # crop using full df stats\n",
    "y_cap = np.percentile(df[\"loc_nonempty\"], 99.0)\n",
    "\n",
    "# --- Plot ---\n",
    "fig = plt.figure(figsize=FIGSIZE)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "hb = ax.hexbin(\n",
    "    x, y, gridsize=55,\n",
    "    extent=(0, x_cap, 0, y_cap),\n",
    "    mincnt=5, bins='log'\n",
    ")\n",
    "\n",
    "# vertical context line\n",
    "ax.axvline(CONTEXT_TOKENS, linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "# overlay tokens/line guide-lines: y = tokens / k  =>  tokens = k * y\n",
    "yy = np.linspace(0, y_cap, 100)\n",
    "ax.plot(k25 * yy, yy, linewidth=0.9, alpha=0.9, label=f\"{k25:.1f} tokens/LOC\")\n",
    "ax.plot(k50 * yy, yy, linewidth=1.2, alpha=1.0, label=f\"{k50:.1f} tokens/LOC (median)\")\n",
    "ax.plot(k75 * yy, yy, linewidth=0.9, alpha=0.9, label=f\"{k75:.1f} tokens/LOC\")\n",
    "\n",
    "# annotate share exceeding 4096 (simple and direct)\n",
    "share_exceed = (df[\"tokens\"] > CONTEXT_TOKENS).mean() * 100\n",
    "y_at_context = (df[\"tokens\"] <= CONTEXT_TOKENS).mean()   # CDF at 4096 for label placement\n",
    "ax.text(CONTEXT_TOKENS * 1.02, min(y_at_context + 0.06, 0.97) * y_cap,  # rough placement\n",
    "        f\">{CONTEXT_TOKENS}: {share_exceed:.1f}%\",\n",
    "        va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# labels/limits\n",
    "ax.set_xlim(0, x_cap)\n",
    "ax.set_ylim(0, y_cap)\n",
    "ax.set_xlabel(\"Tokens\")\n",
    "ax.set_ylabel(\"Non-empty LOC\")\n",
    "ax.set_title(\"Tokens vs. LOC with tokens/line guides (p99 crop)\")\n",
    "\n",
    "# compact colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax, fraction=0.08, pad=0.02)\n",
    "cbar.ax.tick_params(labelsize=7)\n",
    "\n",
    "# lean legend\n",
    "ax.legend(loc=\"lower right\", frameon=False, handlelength=1.8)\n",
    "\n",
    "fig.tight_layout()\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "fig.savefig(OUT_PDF, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[saved] {OUT_PDF}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3916ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge] 514,506 rows\n",
      "[tokens/LOC] 25%=11.79, median=13.06, 75%=14.60\n",
      "[saved] disl_token_stats/hexbin_tokens_vs_loc_guides_ieee.pdf\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Tokens vs. LOC hexbin with tokens/line guide-lines\n",
    "# Legend placed BELOW the axes (outside), IEEE-friendly PDF\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Config (edit paths if needed) ----\n",
    "OUTPUT_DIR          = \"disl_token_stats\"\n",
    "TOKENS_PARQUET_PATH = os.path.join(OUTPUT_DIR, \"lengths_decomposed.parquet\")\n",
    "LINES_PARQUET_PATH  = os.path.join(OUTPUT_DIR,  \"loc_decomposed.parquet\")\n",
    "OUT_PDF             = os.path.join(OUTPUT_DIR,  \"hexbin_tokens_vs_loc_guides_ieee.pdf\")\n",
    "\n",
    "CONTEXT_TOKENS      = 4096\n",
    "SAMPLE_HEXBIN       = 200_000  # set None to use all rows\n",
    "GRIDSIZE            = 55       # hexbin grid\n",
    "MINCNT              = 5        # hide very sparse bins\n",
    "\n",
    "# ---- Load & join ----\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    elif path.endswith(\".jsonl\"):\n",
    "        return pd.read_json(path, lines=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file: {path}\")\n",
    "\n",
    "df_tokens = load_table(TOKENS_PARQUET_PATH)[[\"sha256\", \"tokens\"]]\n",
    "df_lines  = load_table(LINES_PARQUET_PATH)[[\"sha256\", \"loc_nonempty\"]]\n",
    "df = df_tokens.merge(df_lines, on=\"sha256\", how=\"inner\").dropna()\n",
    "df = df[df[\"loc_nonempty\"] > 0].copy()\n",
    "print(f\"[merge] {len(df):,} rows\")\n",
    "\n",
    "# ---- tokens/line stats (for guide-lines) ----\n",
    "df[\"tpl\"] = df[\"tokens\"] / df[\"loc_nonempty\"]\n",
    "k25 = df[\"tpl\"].quantile(0.25)\n",
    "k50 = df[\"tpl\"].median()\n",
    "k75 = df[\"tpl\"].quantile(0.75)\n",
    "print(f\"[tokens/LOC] 25%={k25:.2f}, median={k50:.2f}, 75%={k75:.2f}\")\n",
    "\n",
    "# ---- IEEE single-column style ----\n",
    "FIGSIZE = (3.4, 2.8)  # a bit taller to make room for legend below\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 8,\n",
    "    \"axes.titlesize\": 8,\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"xtick.labelsize\": 7,\n",
    "    \"ytick.labelsize\": 7,\n",
    "    \"legend.fontsize\": 7,\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "})\n",
    "\n",
    "# ---- Prepare data (crop at p99 for readability) ----\n",
    "plot_df = df\n",
    "if SAMPLE_HEXBIN and len(plot_df) > SAMPLE_HEXBIN:\n",
    "    plot_df = plot_df.sample(SAMPLE_HEXBIN, random_state=42)\n",
    "\n",
    "x = plot_df[\"tokens\"].to_numpy()\n",
    "y = plot_df[\"loc_nonempty\"].to_numpy()\n",
    "x_cap = np.percentile(df[\"tokens\"], 99.0)       # crop using full df stats\n",
    "y_cap = np.percentile(df[\"loc_nonempty\"], 99.0)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig = plt.figure(figsize=FIGSIZE)\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "hb = ax.hexbin(\n",
    "    x, y, gridsize=GRIDSIZE, mincnt=MINCNT, bins=\"log\",\n",
    "    extent=(0, x_cap, 0, y_cap)\n",
    ")\n",
    "\n",
    "# Context window\n",
    "ax.axvline(CONTEXT_TOKENS, linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "# Tokens/line guide-lines: tokens = k * LOC  => x = k * y\n",
    "yy = np.linspace(0, y_cap, 100)\n",
    "ln25, = ax.plot(k25 * yy, yy, linewidth=0.9, label=f\"{k25:.1f} tokens/LoC\")\n",
    "ln50, = ax.plot(k50 * yy, yy, linewidth=1.2, label=f\"{k50:.1f} tokens/LoC (median)\")\n",
    "ln75, = ax.plot(k75 * yy, yy, linewidth=0.9, label=f\"{k75:.1f} tokens/LoC\")\n",
    "\n",
    "# Simple overflow annotation\n",
    "share_exceed = (df[\"tokens\"] > CONTEXT_TOKENS).mean() * 100\n",
    "ax.text(CONTEXT_TOKENS * 1.02, 0.85 * y_cap, f\">{CONTEXT_TOKENS}: {share_exceed:.1f}%\",\n",
    "        va=\"bottom\", ha=\"left\")\n",
    "\n",
    "# Axes labels/limits\n",
    "ax.set_xlim(0, x_cap)\n",
    "ax.set_ylim(0, y_cap)\n",
    "ax.set_xlabel(\"Tokens\")\n",
    "ax.set_ylabel(\"Non-empty LoC\")\n",
    "#ax.set_title(\"Tokens vs. LOC with tokens/line guides (p99 crop)\")\n",
    "\n",
    "# Compact colorbar\n",
    "cbar = fig.colorbar(hb, ax=ax, fraction=0.08, pad=0.02)\n",
    "cbar.ax.tick_params(labelsize=7)\n",
    "\n",
    "# ---- Legend BELOW the axes (outside), no overlap ----\n",
    "handles = [ln25, ln50, ln75]\n",
    "leg = ax.legend(\n",
    "    handles=handles,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.22),   # below axes\n",
    "    frameon=False,\n",
    "    ncol=3,\n",
    "    columnspacing=0.9,\n",
    "    handlelength=1.6,\n",
    ")\n",
    "fig.subplots_adjust(bottom=0.30)   # give the legend breathing room\n",
    "\n",
    "# Save IEEE-ready vector PDF\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "fig.savefig(OUT_PDF, format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"[saved] {OUT_PDF}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67917587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
