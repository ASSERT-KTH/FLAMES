{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff78ed4-4509-49af-a7d4-debc4fc4e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843428c2-55aa-4325-a057-1bc5fb83ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af05f291f44547f083dbe23b8dfac384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(\"GGmorello/FLAMES-20k\")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    cache_dir=os.environ.get('TMPDIR'),\n",
    ")\n",
    "ft_model_20 = PeftModel.from_pretrained(model, \"GGmorello/FLAMES-20k\")\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "ft_model_20 = ft_model_20.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47bb4894-ab96-42c1-b953-c67eee54d4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af05f291f44547f083dbe23b8dfac384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(\"/mimer/NOBACKUP/groups/naiss2024-23-121/morello/training\")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    cache_dir=os.environ.get('TMPDIR'),\n",
    ")\n",
    "ft_model_100 = PeftModel.from_pretrained(model, \"/mimer/NOBACKUP/groups/naiss2024-23-121/morello/training\")\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "ft_model_100 = ft_model_100.to('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be780cb1-32ed-497e-afc8-131fdbd6f694",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.tokens.instruct.request import FIMRequest\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "model_id = \"mistralai/Codestral-22B-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=os.environ.get('TMPDIR'),)\n",
    "tokenizer = MistralTokenizer.v3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdedf2d-6105-4829-b82d-508b5e3fff2a",
   "metadata": {},
   "source": [
    "prefix = \"\"\"def add(\"\"\"\n",
    "suffix = \"\"\"    return sum\"\"\"\n",
    "\n",
    "request = FIMRequest(prompt=prefix, suffix=suffix)\n",
    "print(request)\n",
    "\n",
    "tokens = tokenizer.encode_fim(request)\n",
    "t = torch.IntTensor(tokens.tokens)\n",
    "print(tokens)\n",
    "\n",
    "outputs = model.generate(t,max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d624b872-f702-488e-818e-15139f46fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8464e4cbb045f5b24c611635938422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1af03dd73d4cd4aac325f0e25d39eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83980c55dd634c069e7823c4e617c29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d6df03fb9143348bd11a4dacae87f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6891e308134a5993ab4d0dc8e6aa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28424abc77904a239e03e56abce04bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c742ee5f5834647aa4b01eca57089a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pt_model = LlamaForCausalLM.from_pretrained(\n",
    "    'meta-llama/CodeLlama-7b-hf', \n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    ),\n",
    "    cache_dir=os.environ.get('TMPDIR'))\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/CodeLlama-7b-hf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916b583e-6740-4991-a404-8e8e6789ccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3208df00c5d4959a01e6d16ac850403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98aeea2bfbcf46dfa4943352a7d07823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640b95b5a02c4c0c8b01a938760f7722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset('GGMorello/FLAMES', 'distilled_test', split='train', cache_dir=os.environ.get('TMPDIR'), num_proc=64).shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bec51a0-cd4f-4a6c-bb37-4e40ee6ac679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment': '\"LazyBonesSpaceTrip: You can mint a maximum of 20 Lazy Bones\"',\n",
       " 'input': '// SPDX-License-Identifier: MIT\\npragma solidity >=0.4.22 <0.9.0;\\nimport \"@openzeppelin/contracts/token/ERC721/extensions/ERC721Enumerable.sol\";\\nimport \"@openzeppelin/contracts/token/ERC721/ERC721.sol\";\\nimport \"@openzeppelin/contracts/security/Pausable.sol\";\\nimport \"@openzeppelin/contracts/access/AccessControl.sol\";\\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\\nimport \"@openzeppelin/contracts/utils/math/SafeMath.sol\";\\nimport \"@openzeppelin/contracts/utils/Strings.sol\";\\ncontract LazyBonesSpaceTrip is ERC721Enumerable, Ownable, AccessControl {\\n    using SafeMath for uint256;\\n    uint256 public constant PreMintPrice = 0.04 ether;\\n    uint256 public constant PublicMintPrice = 0.06 ether;\\n    uint256 public constant TOTAL_NUMBER_OF_LAZY_BONES = 10000;\\n    uint[TOTAL_NUMBER_OF_LAZY_BONES] internal indices;\\n    uint internal nonce = 0;\\n    uint256 public giveaway_reserved = 100;\\n    uint256 public pre_mint_limit = 3;\\n    mapping(address => bool) private _pre_sale_minters;\\n    bool public mintIsPaused = true;\\n    bool public preMintIsPaused = true;\\n    bool public reveal = false;\\n    //withdraw addresses\\n    address splitter;\\n    //initial team\\n    address LazyDescartes = 0x8D6d8E912880e2fC9dC174F18b033F9619c0F63A;\\n    address LazyGeek = 0x36A59A0B623a4B9EF9d4b4bb11F2aAC40B2dc239;\\n    address LazyDraws = 0x0077044aE65E5E43F10101d9432b763AFdCe540D;\\n    address LazyFlownee = 0x66318D2E71e1DDbE2cC769bacA169463E54B8519;\\n    address LazyElf = 0x5C6b5F156Cb3154442e4B486320A0A5916312C92;\\n    address LazyBrah = 0x09E7C871E020f74D1EE1E30034a90082C435Bece;\\n    address LazyHoodie = 0x797074BC5051d705DFe4004482783381e1ab1222;\\n    //investors\\n    address LazyInvestor1 = 0x694461DCC47900B2716E4c10322e76737733F782;\\n    address LazyInvestor2 = 0xE8e38EB9C16C17681d26522685B381ea659CAc98;\\n    string private _baseUrl = \"https://lazybonesspacetrip.s3.eu-north-1.amazonaws.com/\";\\n    constructor() ERC721(\"LazyBonesSpaceTrip\", \"LBST\") {\\n    }\\n    modifier WhenPreMintIsAllowed() {\\n    }\\n    modifier WhenMintIsAllowed() {\\n    }\\n    //Checks for preminters\\n    function is_pre_mint_allowed(address account) public view  returns (bool) {\\n    }\\n    modifier IsPreMintAllowed(address account) {\\n    }\\n    //EVENTS\\n    event ReedemedPreMint(address account);\\n    //Construction\\n    function _baseURI() internal view virtual override returns (string memory) {\\n    }\\n    function getBaseURI() public view returns (string memory) {\\n    }\\n    function tokenURI(uint256 tokenId) public view virtual override returns (string memory) {\\n    }\\n    function ownerOf(uint8 index) public view returns(address) {\\n    }\\n    function supportsInterface(bytes4 interfaceId) public view override(ERC721Enumerable, AccessControl) returns (bool) {\\n    }\\n    function randomIndex() internal returns (uint256) {\\n    }\\n    fallback() external payable { }\\n    receive() external payable { }\\n    //Minting\\n    function mint(uint256 numberOfTokens) public payable WhenMintIsAllowed() {\\n    }\\n    function preMint(uint256 numberOfTokens) public payable WhenPreMintIsAllowed() IsPreMintAllowed(msg.sender) {\\n        uint256 buyerBalance = balanceOf(msg.sender);\\n        require(msg.value >= numberOfTokens * PreMintPrice, \"LazyBonesSpaceTrip: Ether sent is less than PreMintPrice\");\\n        require(<FILL_ME>)\\n        if(buyerBalance + numberOfTokens >= pre_mint_limit) {\\n            _pre_sale_minters[msg.sender] = false;\\n        }\\n        for(uint256 i; i < numberOfTokens; i++){\\n            _safeMint( msg.sender, randomIndex() );\\n        }\\n    }\\n    //Checks if minter reached limit for public sale\\n    function limitReached(address account, uint256 amount) public view returns(bool) {\\n    }\\n    //Checks if minter in white list\\n    function preMintAvailable(address account) public view returns(bool) {\\n    }\\n    //Checks if minter in giveaway white list\\n    //Only owner\\n    function toggleReveal() public onlyOwner {\\n    }\\n    function addToPreMinters(address[] calldata account) public onlyOwner {\\n    }\\n    function unpauseMint() public onlyOwner {\\n    }\\n    function pauseMint() public onlyOwner {\\n    }\\n    function unpausePreMint() public onlyOwner {\\n    }\\n    function pausePreMint() public onlyOwner {\\n    }\\n    function setBaseURI(string memory baseURI) public onlyOwner {\\n    }\\n    function withdraw() public onlyOwner {\\n    }\\n    function withdrawSplitter() public onlyOwner {\\n    }\\n    function mintToGiftWinners(address[] calldata accounts) public onlyOwner {\\n    }\\n}\\n',\n",
       " 'label': 'numberOfTokens+buyerBalance<=pre_mint_limit,\"LazyBonesSpaceTrip: You can mint a maximum of 20 Lazy Bones\"',\n",
       " 'original_idx': 383327,\n",
       " 'predicate': 'numberOfTokens+buyerBalance<=pre_mint_limit',\n",
       " 'len': -43}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = test_dataset[2]['input']\n",
    "test_dataset[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f8cda54-8305-45ba-8142-3ffb7117b965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payable(t6).send(_each*5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = llama_tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"].to('cuda')\n",
    "generated_ids = ft_model.generate(input_ids, max_new_tokens=128)\n",
    "\n",
    "filling = llama_tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "813dec59-acc0-481a-9af6-13ff224c788f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payable(t6).send(_each * 5));\n",
      "  }\n",
      "  function _mint(address _receiver, uint256 _mintAmount) internal {\n",
      "  }\n",
      "  function _mintForAddress(address _receiver, uint256 _mintAmount) internal {\n",
      "  }\n",
      "  function _setTokenURI(uint256 _tokenId, string memory _uri) internal {\n",
      "  }\n",
      "  function _setTokenURIPrefix(string memory _uriPrefix) internal {\n",
      "  }\n",
      "  function _setTokenURISuffix(string\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = llama_tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"].to('cuda')\n",
    "generated_ids = pt_model.generate(input_ids, max_new_tokens=128)\n",
    "\n",
    "filling = llama_tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50482dae-32df-4618-b05f-bf5f7d34106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3983/5000 [1:07:01<15:24,  1.10it/s]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "results_100 = []\n",
    "for data in tqdm(test_dataset.select(range(5000))):\n",
    "    tok = llama_tokenizer(data['input'], return_tensors='pt').to('cuda')\n",
    "    \n",
    "    generated_ids = ft_model_100.generate(**tok, max_new_tokens=256, pad_token_id=llama_tokenizer.eos_token_id)\n",
    "    ft_filling = llama_tokenizer.batch_decode(generated_ids[:, tok['input_ids'].shape[1]:], skip_special_tokens = True)[0]\n",
    "    results_100.append(ft_filling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1eee0-e789-4c6b-b660-73a78be7181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 84/5000 [01:26<1:13:08,  1.12it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "results_20 = []\n",
    "for data in tqdm(test_dataset.select(range(5000))):\n",
    "    tok = llama_tokenizer(data['input'], return_tensors='pt').to('cuda')\n",
    "    \n",
    "    generated_ids = ft_model_20.generate(**tok, max_new_tokens=256, pad_token_id=llama_tokenizer.eos_token_id)\n",
    "    ft_filling = llama_tokenizer.batch_decode(generated_ids[:, tok['input_ids'].shape[1]:], skip_special_tokens = True)[0]\n",
    "    results_20.append(ft_filling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff6bd5-a540-4d80-8604-c269c57ba075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3012/5000 [3:45:06<2:21:05,  4.26s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "results_pretrained = []\n",
    "for data in tqdm(test_dataset.select(range(5000))):\n",
    "    tok = llama_tokenizer(data['input'], return_tensors='pt').to('cuda')\n",
    "    \n",
    "    generated_ids = pt_model.generate(**tok, max_new_tokens=128, pad_token_id=llama_tokenizer.eos_token_id)\n",
    "    pt_filling = llama_tokenizer.batch_decode(generated_ids[:, tok['input_ids'].shape[1]:], skip_special_tokens = True)[0]\n",
    "    results_pretrained.append(pt_filling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62037d-7357-4b02-b7a2-4feeafd6f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_100 +=  [''] * (len(test_dataset) - len(results_100))\n",
    "\n",
    "test_dataset = test_dataset.add_column('results', results_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31566dea-97bc-4497-bac5-996fd9532910",
   "metadata": {},
   "outputs": [],
   "source": [
    "results += [''] * (len(test_dataset) - len(results))\n",
    "\n",
    "test_dataset = test_dataset.add_column('results', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d57e0-95bc-4ec3-8f34-370ac8efa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pretrained += [''] * (len(test_dataset) - len(results_pretrained))\n",
    "\n",
    "test_dataset = test_dataset.add_column('results CodeLLama', results_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c77d58-65d0-460b-ba4c-72d2fb7e8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.push_to_hub('GGmorello/FLAMES_results', data_dir='data/Codellama_pretrained_5k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174e3bc-02d9-41c6-bd6a-941b0070da20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
